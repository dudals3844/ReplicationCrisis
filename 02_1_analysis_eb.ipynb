{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "from settings import settings \n",
    "from sklearn.utils import resample\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = {\n",
    "    \"us\": [\"us\", \"hml\", 2],\n",
    "    \"developed\": [\"developed\", \"hml\", 2],\n",
    "    \"emerging\": [\"emerging\", \"hml\", 2],\n",
    "    \"all\": [[\"us\", \"developed\", \"emerging\"], \"hml\", 3],\n",
    "    \"world\": [\"world\", \"hml\", 2],\n",
    "    \"world_ex_us\": [\"world_ex_us\", \"hml\", 2],\n",
    "    \"us_mega\": [\"us\", \"cmp\", 2, \"mega\"],\n",
    "    \"us_large\": [\"us\", \"cmp\", 2, \"large\"],\n",
    "    \"us_small\": [\"us\", \"cmp\", 2, \"small\"],\n",
    "    \"us_micro\": [\"us\", \"cmp\", 2, \"micro\"],\n",
    "    \"us_nano\": [\"us\", \"cmp\", 2, \"nano\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_pfs_cmp = pd.read_parquet(data_path / \"regional_pfs_cmp.parquet\") \n",
    "regional_pfs = pd.read_parquet(data_path / \"regional_pfs.parquet\")\n",
    "cluster_labels = pd.read_parquet(data_path / \"cluster_labels.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the empirical Bayes estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eb_prepare(data, scale_alphas, overlapping):\n",
    "    if overlapping:\n",
    "        data['obs'] = data.groupby(['region', 'characteristic'])['region'].transform('size')\n",
    "        data = data.loc[data.groupby(['region', 'characteristic'])['obs'].idxmax()]\n",
    "        data = data.drop(columns=['obs'])\n",
    "\n",
    "    data = data.copy()\n",
    "    data.loc[:, \"ret_neu\"] = (\n",
    "        data\n",
    "        .groupby(['region', 'characteristic'], group_keys=False)[[\"ret\", \"mkt_vw_exc\"]]\n",
    "        .apply(lambda x: x[\"ret\"] - x['ret'].cov(x[\"mkt_vw_exc\"]) / x[\"mkt_vw_exc\"].var() * x[\"mkt_vw_exc\"])\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    data.loc[:, \"ret_neu\"] *= 100\n",
    "\n",
    "    scaling_fct = np.sqrt(10**2 / 12) / data.groupby(['region', 'characteristic'])['ret_neu'].transform('std')\n",
    "    data.loc[:, 'ret_neu_scaled'] = data['ret_neu'] * scaling_fct\n",
    "    data['name_wide'] = data['characteristic'] + '__' + data['region']\n",
    "\n",
    "    if scale_alphas:\n",
    "        data_wide = data.pivot(index='eom', columns='name_wide', values='ret_neu_scaled')\n",
    "    else:\n",
    "        data_wide = data.pivot(index='eom', columns='name_wide', values='ret_neu')\n",
    "    return {\n",
    "        \"long\": data, \n",
    "        \"wide\": data_wide\n",
    "    }\n",
    "\n",
    "\n",
    "def block_cluster_func(cor_mat: pd.DataFrame, cl_labels: pd.DataFrame):\n",
    "    cor_mat = cor_mat.copy()\n",
    "    cor_mat.index.name = \"index\"\n",
    "    cl_labels = cl_labels.copy()\n",
    "\n",
    "    __cor_long = cor_mat.reset_index().melt(id_vars='index', var_name='char2', value_name='cor') \n",
    "    # char: 요인이름과 region을 분리\n",
    "    __cor_long[['char2', 'region2']] = __cor_long['char2'].str.split('__', expand=True)\n",
    "    __cor_long[['char1', 'region1']] = __cor_long['index'].str.split('__', expand=True)\n",
    "\n",
    "    # 요인별 cluster 이름을 추가\n",
    "    __cor_long = __cor_long.merge(cl_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl1'}), left_on='char1', right_on='characteristic', how='left')\n",
    "    __cor_long = __cor_long.merge(cl_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl2'}), left_on='char2', right_on='characteristic', how='left')\n",
    "\n",
    "    # 개별 요인이 포함돼 있는 클러스터와 region을 합침\n",
    "    __cor_long['hclreg1'] = __cor_long['hcl1'] + '__' + __cor_long['region1']\n",
    "    __cor_long['hclreg2'] = __cor_long['hcl2'] + '__' + __cor_long['region2']\n",
    "\n",
    "    # Create hcl_pair column\n",
    "\n",
    "    __cor_long['hcl_pair'] = __cor_long.apply(lambda row: '_x_'.join(sorted([row['hclreg1'], row['hclreg2']])), axis=1)    \n",
    "    __cor_long['name1'] = __cor_long['char1'] + '__' + __cor_long['region1']\n",
    "    __cor_long['name2'] = __cor_long['char2'] + '__' + __cor_long['region2']\n",
    "\n",
    "    # 같은 thema안에서 correlation의 평균\n",
    "    __cluster_wise_cor = __cor_long[__cor_long['name1'] != __cor_long['name2']].groupby('hcl_pair')['cor'].mean().reset_index(name='cor_avg')    \n",
    "    __cor_long = __cor_long.merge(__cluster_wise_cor, on='hcl_pair', how='left') \n",
    "    __cor_long['cor_avg'] = np.where(__cor_long['name1'] == __cor_long['name2'], 1, __cor_long['cor_avg']) \n",
    "    __cluster_block_cor_matrix = __cor_long.pivot(index='name1', columns='name2', values='cor_avg') \n",
    "    return __cluster_block_cor_matrix     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_bayes(\n",
    "        data: pd.DataFrame,\n",
    "        cluster_labels: pd.DataFrame, \n",
    "        min_obs=5*12, \n",
    "        fix_alpha=False,\n",
    "        bs_cov=False,\n",
    "        cor_type=\"sample\",\n",
    "        shrinkage=0,\n",
    "        layers=3,\n",
    "        bs_samples=10000,\n",
    "        seed=None,\n",
    "        priors=None,\n",
    "        sigma=None,\n",
    "    ):\n",
    "    np.random.seed(seed)\n",
    "    y_raw = data[\"wide\"].copy()\n",
    "\n",
    "    # 최소 개수 제한\n",
    "    obs = y_raw.notna().sum()\n",
    "    y = y_raw.loc[:, obs[obs >= min_obs].index]\n",
    "    n_fcts = len(y.columns)\n",
    "\n",
    "    y_mean = y.mean()\n",
    "\n",
    "    if sigma is None:\n",
    "        if bs_cov:\n",
    "            bs_samples_list = []\n",
    "            for i in tqdm(range(bs_samples)):\n",
    "                # 행을 중복을 허용하면서 sample하면서 mean을 계산\n",
    "                sample = resample(y, replace=True)\n",
    "                bs_samples_list.append(sample.mean())\n",
    "\n",
    "            bs_full = pd.DataFrame(bs_samples_list)\n",
    "            bs_full_cov = bs_full.cov()\n",
    "\n",
    "            alpha_sd = pd.Series(np.sqrt(np.diag(bs_full_cov)), index=y_mean.index)\n",
    "            alpha_cor = bs_full.corr()\n",
    "        else:\n",
    "            y_sd = pd.Series(np.nanstd(y, axis=0), index=y.columns)\n",
    "            alpha_sd = y_sd / np.sqrt(y.shape[0])\n",
    "            alpha_cor = y.corr()\n",
    "\n",
    "        alpha_cor_shrunk = (\n",
    "            alpha_cor * (1 - shrinkage) + np.diag(np.full(n_fcts, 1)) * shrinkage\n",
    "        )\n",
    "        if cor_type == \"sample\":\n",
    "            alpha_cor_adj = alpha_cor_shrunk\n",
    "        elif cor_type == \"block_clusters\":\n",
    "            alpha_cor_adj = block_cluster_func(alpha_cor_shrunk, cl_labels=cluster_labels)\n",
    "        __corr = np.diag(alpha_sd) @ alpha_cor_adj @ np.diag(alpha_sd)\n",
    "        sigma = pd.DataFrame(\n",
    "            __corr.values, index=alpha_cor_adj.columns, columns=alpha_cor_adj.columns\n",
    "        )\n",
    "    else:\n",
    "        alpah_sd = np.sqrt(np.diag(sigma))\n",
    "\n",
    "    cm = y_mean.to_frame(\"value\")\n",
    "    cm.index.name = \"char_reg\"\n",
    "    cm = cm.reset_index()\n",
    "    cm[\"characteristic\"] = cm[\"char_reg\"].str.split(\"__\").str[0]\n",
    "    cm = cm.merge(cluster_labels, on=\"characteristic\", how=\"left\")\n",
    "\n",
    "    # Factor가 Cluster 어디에 포함돼 있는지 나타내는 matrix\n",
    "    m = (\n",
    "        cm.assign(cm=1)[[\"char_reg\", \"hcl_label\", \"cm\"]]\n",
    "        .pivot(index=\"char_reg\", columns=\"hcl_label\", values=\"cm\")\n",
    "        .fillna(0)\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    mm = m @ m.T\n",
    "    # Cluster 개수\n",
    "    n_cl = m.shape[1]\n",
    "\n",
    "    z = (\n",
    "        cm.assign(sm=1)[[\"char_reg\", \"characteristic\", \"sm\"]]\n",
    "        .pivot(index=\"char_reg\", columns=\"characteristic\", values=\"sm\")\n",
    "        .fillna(0)\n",
    "        .copy()\n",
    "    )\n",
    "    zz = z @ z.T\n",
    "    # 개별 factor의 개수\n",
    "    n_s = z.shape[1]\n",
    "\n",
    "    starting_values = (\n",
    "        cm.groupby([\"hcl_label\", \"characteristic\"])\n",
    "        .agg(\n",
    "            n_s=(\"value\", \"size\"),  # Count of rows per group\n",
    "            signal_mean=(\"value\", \"mean\"),  # Mean of the 'value' column\n",
    "            signal_sd=(\"value\", \"std\"),  # Standard deviation of the 'value' column\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    starting_values = (\n",
    "        starting_values.groupby(\"hcl_label\")\n",
    "        .agg(\n",
    "            n_c=(\"n_s\", \"sum\"),  # Sum of n_s within each cluster\n",
    "            cl_mean=(\"signal_mean\", \"mean\"),  # Mean of signal means\n",
    "            cl_sd=(\"signal_mean\", \"std\"),  # Standard deviation of signal means\n",
    "            cl_signal_within=(\"signal_sd\", \"mean\"),  # Mean of signal standard deviations\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Cluster에 포함된 factor개수가 1개일때 cl_sd를 0으로 설정\n",
    "    starting_values[\"cl_sd\"] = np.where(\n",
    "        starting_values[\"n_c\"] == 1, 0, starting_values[\"cl_sd\"]\n",
    "    )\n",
    "\n",
    "    alpha_mean = starting_values[\"cl_mean\"].mean()\n",
    "    sd_cl_mean = (\n",
    "        np.sqrt(np.sum(starting_values[\"cl_mean\"] ** 2)) / (len(starting_values) - 1)\n",
    "        if fix_alpha\n",
    "        else starting_values[\"cl_mean\"].std()\n",
    "    )\n",
    "    sd_within_cl = starting_values[\"cl_sd\"].mean()\n",
    "    sd_within_signal = starting_values[\"cl_signal_within\"].mean() \n",
    "\n",
    "\n",
    "    starting_values = {\n",
    "        \"alpha_mean\": alpha_mean,\n",
    "        \"sd_cl_mean\": sd_cl_mean,\n",
    "        \"sd_within_cl\": sd_within_cl,\n",
    "        \"sd_within_signal\": sd_within_signal,\n",
    "    }\n",
    "\n",
    "    if fix_alpha:\n",
    "        sd_all = np.sqrt(y_mean.pow(2).sum()) / (len(y_mean) - 1)\n",
    "    else:\n",
    "        sd_all = y_mean.std()\n",
    "\n",
    "\n",
    "    def omega_func(layers, tau_c, tau_s=None, tau_w=None):\n",
    "        # Initialize the diagonal matrix based on the number of factors (n_fcts)\n",
    "        if layers == 1:\n",
    "            # All alphas are drawn from the same distribution\n",
    "            a_omega = np.eye(n_fcts) * tau_c**2\n",
    "        elif layers == 2:\n",
    "            # All cluster alphas are drawn from the same distribution\n",
    "            a_omega = np.eye(n_fcts) * tau_s**2 + mm * tau_c**2\n",
    "        elif layers == 3:\n",
    "            # Cluster distribution, signal distribution, and factor distribution\n",
    "            a_omega = np.eye(n_fcts) * tau_w**2 + zz * tau_s**2 + mm * tau_c**2\n",
    "        else:\n",
    "            raise ValueError(\"layers should be 1, 2, or 3.\")\n",
    "\n",
    "        return a_omega\n",
    "    \n",
    "    if priors is None:\n",
    "        if layers == 1:\n",
    "            start_list = {\n",
    "                'a': starting_values['alpha_mean'],\n",
    "                'tc': sd_all\n",
    "            }\n",
    "\n",
    "            def mle_func(params):\n",
    "                a, tc = params\n",
    "                a_vec = np.full(n_fcts, a)\n",
    "                a_omega = omega_func(layers=layers, tau_c=tc, tau_s=None, tau_w=None)\n",
    "                a_cov = sigma + a_omega\n",
    "                return -multivariate_normal.logpdf(y_mean, mean=a_vec, cov=a_cov)\n",
    "\n",
    "        elif layers == 2:\n",
    "            start_list = {\n",
    "                'a': starting_values['alpha_mean'],\n",
    "                'tc': starting_values['sd_cl_mean'],\n",
    "                'ts': starting_values['sd_within_cl']\n",
    "            }\n",
    "\n",
    "            def mle_func(params):\n",
    "                a, tc, ts = params\n",
    "                a_vec = np.full(n_fcts, a)\n",
    "                a_omega = omega_func(layers=layers, tau_c=tc, tau_s=ts, tau_w=None)\n",
    "                a_cov = sigma + a_omega\n",
    "                return -multivariate_normal.logpdf(y_mean, mean=a_vec, cov=a_cov)\n",
    "\n",
    "        elif layers == 3:\n",
    "            start_list = {\n",
    "                'a': starting_values['alpha_mean'],\n",
    "                'tc': starting_values['sd_cl_mean'],\n",
    "                'ts': starting_values['sd_within_cl'],\n",
    "                'tw': starting_values['sd_within_signal']\n",
    "            }\n",
    "\n",
    "            def mle_func(params):\n",
    "                a, tc, ts, tw = params\n",
    "                a_vec = np.full(n_fcts, a)\n",
    "                a_omega = omega_func(layers=layers, tau_c=tc, tau_s=ts, tau_w=tw)\n",
    "                a_cov = sigma + a_omega\n",
    "                return -multivariate_normal.logpdf(y_mean, mean=a_vec, cov=a_cov)\n",
    "\n",
    "        result = minimize(mle_func, list(start_list.values()), bounds=[(-np.inf, None)] + [(0, None)] * (len(start_list) - 1))\n",
    "        assert result.success \n",
    "        # Extract final values\n",
    "        mu = result.x[0]\n",
    "        tau_c = result.x[1]\n",
    "        tau_s = result.x[2] if layers > 1 else None\n",
    "        tau_w = result.x[3] if layers == 3 else None\n",
    "    else:\n",
    "        # Use priors\n",
    "        mu = priors['alpha']\n",
    "        tau_c = priors['tau_c']\n",
    "        tau_s = priors['tau_s']\n",
    "        tau_w = priors['tau_w']\n",
    "\n",
    "\n",
    "    omega = omega_func(layers=layers, tau_c=tau_c, tau_s=tau_s, tau_w=tau_w)\n",
    "    print(f\"Condition Number Omega = {np.round(np.linalg.cond(omega), 2)}\")\n",
    "\n",
    "    if layers == 3:\n",
    "        z_transpose = z.T  # Transpose of matrix z\n",
    "        theta_sigma_inv = np.linalg.inv(omega + sigma)\n",
    "        as_mean = tau_w**2 * z_transpose @ theta_sigma_inv @ (y_mean - np.full(n_fcts, mu))\n",
    "        as_cov = tau_w**2 * np.eye(n_s) - tau_w**4 * z_transpose @ theta_sigma_inv @ z\n",
    "        as_sd = np.sqrt(np.diag(as_cov))\n",
    "\n",
    "        # Convert as_mean and as_sd to DataFrame\n",
    "        as_mean_df = pd.DataFrame(as_mean, index=[z.columns], columns=[\"post_mean\"])\n",
    "        as_sd_df = pd.DataFrame(as_sd, index=[z.columns], columns=[\"post_sd\"])\n",
    "\n",
    "        # Merge as_mean and as_sd into signal_summary\n",
    "        signal_summary = pd.merge(as_mean_df.reset_index(), as_sd_df.reset_index(), on=\"index\", how=\"left\")\n",
    "        signal_summary.rename(columns={\"index\": \"characteristic\"}, inplace=True)\n",
    "\n",
    "    omega_inv = np.linalg.inv(omega)\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "\n",
    "    ai_cov = np.linalg.inv(omega_inv + sigma_inv)\n",
    "    ai_sd = np.sqrt(np.diag(ai_cov))\n",
    "\n",
    "    ai_mean = ai_cov @ (omega_inv @ np.full(n_fcts, mu) + sigma_inv @ y_mean)\n",
    "    ai_mean = pd.Series(ai_mean, index=y_mean.index)\n",
    "    ai_sd = pd.Series(ai_sd, index=y_mean.index)\n",
    "    ai_cov = pd.DataFrame(ai_cov, index=y_mean.index, columns=y_mean.index)\n",
    "\n",
    "    factor_summary = pd.concat([\n",
    "        ai_mean.to_frame(\"post_mean\"), \n",
    "        ai_sd.to_frame(\"post_sd\"), \n",
    "        y_mean.to_frame(\"osl_est\"), \n",
    "        alpha_sd.to_frame(\"osl_se\"), \n",
    "    ], axis=1).reset_index()\n",
    "\n",
    "\n",
    "    factor_summary[\"characteristic\"] = factor_summary[\"char_reg\"].str.split(\"__\").str[0]\n",
    "    factor_summary[\"p025\"] = factor_summary[\"post_mean\"] - 1.96 * factor_summary[\"post_sd\"]\n",
    "    factor_summary[\"p975\"] = factor_summary[\"post_mean\"] + 1.96 * factor_summary[\"post_sd\"]\n",
    "\n",
    "    # Merget Cluster Label\n",
    "    factor_summary = factor_summary.merge(cluster_labels, on=\"characteristic\", how=\"left\")\n",
    "    factor_summary[\"region\"] = factor_summary[\"char_reg\"].str.extract(r\"__(.*)\")\n",
    "\n",
    "    if priors is None:\n",
    "        comparison = pd.DataFrame({\n",
    "            \"estimate\": [\"alpha\", \"tau_c\", \"tau_s\", \"tau_w\"][:layers + 1],\n",
    "            \"crude\": pd.Series(start_list).values,\n",
    "            \"ml_est\": [mu, tau_c, tau_s, tau_w][:layers + 1]\n",
    "        })\n",
    "\n",
    "    ret_list = {\n",
    "        \"input\": data,\n",
    "        \"factors\": factor_summary,\n",
    "        \"factor_mean\": ai_mean,\n",
    "        \"factor_cov\": ai_cov,\n",
    "        \"theta\": omega,\n",
    "        \"sigma\": sigma\n",
    "    }\n",
    "\n",
    "    if sigma is None:\n",
    "        ret_list[\"alpha_cor_raw\"] = alpha_cor_shrunk\n",
    "        ret_list[\"alpha_cor_adj\"] = alpha_cor_adj\n",
    "\n",
    "    if priors is None:\n",
    "        ret_list[\"mle\"] = comparison\n",
    "\n",
    "    if layers == 3:\n",
    "        ret_list[\"signal\"] = signal_summary\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1857.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 5.24\n",
      "Region: developed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3746.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 5.04\n",
      "Region: emerging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3492.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 4.88\n",
      "Region: ['us', 'developed', 'emerging']\n",
      "Region: world\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1840.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 3.44\n",
      "Region: world_ex_us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3658.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 5.45\n",
      "Region: us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1828.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 6.22\n",
      "Region: us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1874.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 6.59\n",
      "Region: us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1924.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 7.77\n",
      "Region: us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1866.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 4.43\n",
      "Region: us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2425.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number Omega = 2.23\n"
     ]
    }
   ],
   "source": [
    "eb_est = {}\n",
    "for key, x in search_list.items():\n",
    "    print(f\"Region: {x[0]}\")\n",
    "    regions = x[0]\n",
    "    \n",
    "    # Select the appropriate data\n",
    "    if x[1] == \"cmp\":\n",
    "        base_data = regional_pfs_cmp[regional_pfs_cmp['size_grp'] == x[3]].copy()\n",
    "    elif x[1] == \"hml\":\n",
    "        base_data = regional_pfs.copy()\n",
    "\n",
    "    if isinstance(regions, str):\n",
    "        regions = [regions]\n",
    "\n",
    "    if x[2] == 3:\n",
    "        # 에러가 았어서 pass\n",
    "        continue\n",
    "\n",
    "    data = base_data[(base_data['eom'] >= settings['start_date']) & (base_data['eom'] <= settings['end_date']) & (base_data['region'].isin(regions))]\n",
    "    data = eb_prepare(data, scale_alphas=settings['eb']['scale_alpha'], overlapping=settings['eb']['overlapping'])\n",
    "    op = emp_bayes(\n",
    "        data=data, \n",
    "        cluster_labels=cluster_labels, \n",
    "        min_obs=settings['eb']['min_obs'], \n",
    "        fix_alpha=settings['eb']['fix_alpha'], \n",
    "        bs_cov=settings['eb']['bs_cov'],\n",
    "        layers=x[2], \n",
    "        shrinkage=settings['eb']['shrinkage'], \n",
    "        cor_type=settings['eb']['cor_type'], \n",
    "        bs_samples=settings['eb']['bs_samples'], \n",
    "        seed=settings['seed'], \n",
    "        priors=None, \n",
    "        sigma = None,\n",
    "    )\n",
    "    op[\"input\"] = data\n",
    "    eb_est[key] = op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Estimate 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path / \"eb_est.pkl\", \"wb\") as f:\n",
    "    pickle.dump(eb_est, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
