{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "from settings import settings \n",
    "from sklearn.utils import resample\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = {\n",
    "    \"us\": [\"us\", \"hml\", 2],\n",
    "    \"developed\": [\"developed\", \"hml\", 2],\n",
    "    \"emerging\": [\"emerging\", \"hml\", 2],\n",
    "    \"all\": [[\"us\", \"developed\", \"emerging\"], \"hml\", 3],\n",
    "    \"world\": [\"world\", \"hml\", 2],\n",
    "    \"world_ex_us\": [\"world_ex_us\", \"hml\", 2],\n",
    "    \"us_mega\": [\"us\", \"cmp\", 2, \"mega\"],\n",
    "    \"us_large\": [\"us\", \"cmp\", 2, \"large\"],\n",
    "    \"us_small\": [\"us\", \"cmp\", 2, \"small\"],\n",
    "    \"us_micro\": [\"us\", \"cmp\", 2, \"micro\"],\n",
    "    \"us_nano\": [\"us\", \"cmp\", 2, \"nano\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_pfs_cmp = pd.read_parquet(data_path / \"regional_pfs_cmp.parquet\") \n",
    "regional_pfs = pd.read_parquet(data_path / \"regional_pfs.parquet\")\n",
    "cluster_labels = pd.read_parquet(data_path / \"cluster_labels.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the empirical Bayes estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eb_prepare(data, scale_alphas, overlapping):\n",
    "    if overlapping:\n",
    "        data['obs'] = data.groupby(['region', 'characteristic'])['region'].transform('size')\n",
    "        data = data.loc[data.groupby(['region', 'characteristic'])['obs'].idxmax()]\n",
    "        data = data.drop(columns=['obs'])\n",
    "\n",
    "    data = data.copy()\n",
    "    data.loc[:, \"ret_neu\"] = (\n",
    "        data\n",
    "        .groupby(['region', 'characteristic'], group_keys=False)[[\"ret\", \"mkt_vw_exc\"]]\n",
    "        .apply(lambda x: x[\"ret\"] - x['ret'].cov(x[\"mkt_vw_exc\"]) / x[\"mkt_vw_exc\"].var() * x[\"mkt_vw_exc\"])\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    data.loc[:, \"ret_neu\"] *= 100\n",
    "\n",
    "    scaling_fct = np.sqrt(10**2 / 12) / data.groupby(['region', 'characteristic'])['ret_neu'].transform('std')\n",
    "    data.loc[:, 'ret_neu_scaled'] = data['ret_neu'] * scaling_fct\n",
    "    data['name_wide'] = data['characteristic'] + '__' + data['region']\n",
    "\n",
    "    if scale_alphas:\n",
    "        data_wide = data.pivot(index='eom', columns='name_wide', values='ret_neu_scaled')\n",
    "    else:\n",
    "        data_wide = data.pivot(index='eom', columns='name_wide', values='ret_neu')\n",
    "    return {\n",
    "        \"long\": data, \n",
    "        \"wide\": data_wide\n",
    "    }\n",
    "\n",
    "\n",
    "def block_cluster_func(cor_mat: pd.DataFrame, cl_labels: pd.DataFrame):\n",
    "    cor_mat = cor_mat.copy()\n",
    "    cor_mat.index.name = \"index\"\n",
    "    cl_labels = cl_labels.copy()\n",
    "\n",
    "    __cor_long = cor_mat.reset_index().melt(id_vars='index', var_name='char2', value_name='cor') \n",
    "    # char: 요인이름과 region을 분리\n",
    "    __cor_long[['char2', 'region2']] = __cor_long['char2'].str.split('__', expand=True)\n",
    "    __cor_long[['char1', 'region1']] = __cor_long['index'].str.split('__', expand=True)\n",
    "\n",
    "    # 요인별 cluster 이름을 추가\n",
    "    __cor_long = __cor_long.merge(cl_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl1'}), left_on='char1', right_on='characteristic', how='left')\n",
    "    __cor_long = __cor_long.merge(cl_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl2'}), left_on='char2', right_on='characteristic', how='left')\n",
    "\n",
    "    # 개별 요인이 포함돼 있는 클러스터와 region을 합침\n",
    "    __cor_long['hclreg1'] = __cor_long['hcl1'] + '__' + __cor_long['region1']\n",
    "    __cor_long['hclreg2'] = __cor_long['hcl2'] + '__' + __cor_long['region2']\n",
    "\n",
    "    # Create hcl_pair column\n",
    "\n",
    "    __cor_long['hcl_pair'] = __cor_long.apply(lambda row: '_x_'.join(sorted([row['hclreg1'], row['hclreg2']])), axis=1)    \n",
    "    __cor_long['name1'] = __cor_long['char1'] + '__' + __cor_long['region1']\n",
    "    __cor_long['name2'] = __cor_long['char2'] + '__' + __cor_long['region2']\n",
    "\n",
    "    # 같은 thema안에서 correlation의 평균\n",
    "    __cluster_wise_cor = __cor_long[__cor_long['name1'] != __cor_long['name2']].groupby('hcl_pair')['cor'].mean().reset_index(name='cor_avg')    \n",
    "    __cor_long = __cor_long.merge(__cluster_wise_cor, on='hcl_pair', how='left') \n",
    "    __cor_long['cor_avg'] = np.where(__cor_long['name1'] == __cor_long['name2'], 1, __cor_long['cor_avg']) \n",
    "    __cluster_block_cor_matrix = __cor_long.pivot(index='name1', columns='name2', values='cor_avg') \n",
    "    return __cluster_block_cor_matrix     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us\n"
     ]
    }
   ],
   "source": [
    "eb_est = {}\n",
    "for key, x in search_list.items():\n",
    "    print(f\"Region: {x[0]}\")\n",
    "    regions = x[0]\n",
    "    \n",
    "    # Select the appropriate data\n",
    "    if x[1] == \"cmp\":\n",
    "        base_data = regional_pfs_cmp[regional_pfs_cmp['size_grp'] == x[3]].copy()\n",
    "    elif x[1] == \"hml\":\n",
    "        base_data = regional_pfs.copy()\n",
    "\n",
    "    if isinstance(regions, str):\n",
    "        regions = [regions]\n",
    "\n",
    "    data = base_data[(base_data['eom'] >= settings['start_date']) & (base_data['eom'] <= settings['end_date']) & (base_data['region'].isin(regions))]\n",
    "    data = eb_prepare(data, scale_alphas=settings['eb']['scale_alpha'], overlapping=settings['eb']['overlapping'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs=settings['eb']['min_obs']\n",
    "fix_alpha=settings['eb']['fix_alpha']\n",
    "bs_cov=settings['eb']['bs_cov']\n",
    "layers=x[2]\n",
    "shrinkage=settings['eb']['shrinkage']\n",
    "cor_type=settings['eb']['cor_type']\n",
    "bs_samples=settings['eb']['bs_samples']\n",
    "seed=settings['seed']\n",
    "sigma = None\n",
    "priors = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step By Step: Empyrical Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw = data[\"wide\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최소 개수 제한 \n",
    "obs = y_raw.notna().sum()\n",
    "y = y_raw.loc[:, obs[obs >= min_obs].index]\n",
    "n_fcts = len(y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1797.12it/s]\n"
     ]
    }
   ],
   "source": [
    "if sigma is None:\n",
    "    if bs_cov:\n",
    "        bs_samples_list = []\n",
    "        for i in tqdm(range(bs_samples)):\n",
    "            # 행을 중복을 허용하면서 sample하면서 mean을 계산\n",
    "            sample = resample(y, replace=True)\n",
    "            bs_samples_list.append(sample.mean())\n",
    "\n",
    "        bs_full = pd.DataFrame(bs_samples_list)\n",
    "        bs_full_cov = bs_full.cov()\n",
    "\n",
    "        alpha_sd = pd.Series(np.sqrt(np.diag(bs_full_cov)), index=y_mean.index) \n",
    "        alpha_cor = bs_full.corr()\n",
    "    else:\n",
    "        y_sd = pd.Series(np.nanstd(y, axis=0), index=y.columns) \n",
    "        alpha_sd = y_sd / np.sqrt(y.shape[0]) \n",
    "        alpha_cor = y.corr()\n",
    "    \n",
    "    alpha_cor_shrunk = alpha_cor * (1-shrinkage) + np.diag(np.full(n_fcts, 1)) * shrinkage \n",
    "    if cor_type == \"sample\":\n",
    "        alpha_cor_adj = alpha_cor_shrunk\n",
    "    elif cor_type == \"block_clusters\":\n",
    "        alpha_cor_adj = block_cluster_func(alpha_cor_shrunk, cluster_labels=cluster_labels)\n",
    "    __corr = np.diag(alpha_sd) @ alpha_cor_adj @ np.diag(alpha_sd)\n",
    "    sigma = pd.DataFrame(__corr.values, index=alpha_cor_adj.columns, columns=alpha_cor_adj.columns)\n",
    "else:\n",
    "    alpah_sd = np.sqrt(np.diag(sigma))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name_wide\n",
       "age__us                 -0.153475\n",
       "aliq_at__us              0.312448\n",
       "aliq_mat__us            -0.352743\n",
       "ami_126d__us             0.050272\n",
       "at_be__us               -0.158214\n",
       "                           ...   \n",
       "turnover_var_126d__us   -0.128605\n",
       "z_score__us             -0.003803\n",
       "zero_trades_126d__us     0.376804\n",
       "zero_trades_21d__us      0.189812\n",
       "zero_trades_252d__us     0.440058\n",
       "Length: 153, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
