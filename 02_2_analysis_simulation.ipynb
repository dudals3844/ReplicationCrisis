{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import multiprocessing \n",
    "from pathlib import Path \n",
    "from scipy.stats import norm\n",
    "from numpy.linalg import inv\n",
    "from settings import settings \n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = pd.read_parquet(data_path / \"cluster_labels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path / \"eb_est.pkl\", \"rb\") as f:\n",
    "    eb_est = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations EB vs BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_cor = eb_est['us']['input']['long'].pivot_table(index='eom', columns='characteristic', values='ret_neu').corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_cor = pairwise_cor.stack().to_frame(\"cor\")\n",
    "pairwise_cor.index.names = [\"level_0\", \"level_1\"]\n",
    "cor_value = pairwise_cor.reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각 factor별 cluster 이름 merge\n",
    "cor_value = cor_value.merge(cluster_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl1'}), left_on='level_0', right_on='characteristic', how='left')\n",
    "cor_value = cor_value.merge(cluster_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl2'}), left_on='level_1', right_on='characteristic', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_value['same_cluster'] = (cor_value['hcl1'] == cor_value['hcl2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_value_avg = cor_value.groupby('same_cluster')['cor'].mean().reset_index(name='avg_cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 cluster와 다른 cluster간에 상관관계 평균\n",
    "cor_value_avg = cor_value.groupby('same_cluster')['cor'].mean().reset_index(name='avg_cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_months = eb_est['us']['input']['long'].groupby('characteristic')['eom'].count().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sim = {\n",
    "    'yrs': round(med_months / 12),\n",
    "    'cor_within': round(cor_value_avg[cor_value_avg['same_cluster'] == True]['avg_cor'].values[0], 2),\n",
    "    'cor_across': round(cor_value_avg[cor_value_avg['same_cluster'] == False]['avg_cor'].values[0], 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(settings['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_c_list = [0.01] + list(np.arange(0.05, 0.55, 0.05))\n",
    "tau_w_list =  [0.01, 0.2]\n",
    "\n",
    "sim = {\n",
    "    # alpha는 0으로 가정\n",
    "    \"alpha_0\": 0,\n",
    "    \"t\": 12 * 70,\n",
    "    \"clusters\": 13,\n",
    "    # cluster 안에 요인 개수 \n",
    "    \"fct_pr_cl\": 10,\n",
    "    \"corr_within\": 0.58,\n",
    "    \"corr_across\": 0.02,\n",
    "    \"n_sims\": 10000,\n",
    "}\n",
    "sim['se'] = (10 / np.sqrt(12)) / np.sqrt(sim['t'])\n",
    "sim['n'] = sim['clusters'] * sim['fct_pr_cl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_list = list(product(tau_c_list, tau_w_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Multiple Testing Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_mt_control(tau_c, tau_w, sim_settings):\n",
    "    # Cluster membership matrix\n",
    "    m = np.zeros((sim_settings['n'], sim_settings['clusters']))\n",
    "    j = 0\n",
    "\n",
    "    # 130개 요인 X 13개 Cluster\n",
    "    for i in range(sim_settings['clusters']):\n",
    "        m[j:j + sim_settings['fct_pr_cl'], i] = 1\n",
    "        j += sim_settings['fct_pr_cl']\n",
    "\n",
    "    \n",
    "    \n",
    "    # 같은 클러스터에 속하는 요인들의 matrix를 임시로 correlation matrix로 사용\n",
    "    corr_mat = m @ m.T\n",
    "    # 다른 Factor 클러스터와 상관관계는 corr_across로 대체하고 \n",
    "    corr_mat[corr_mat == 0] = sim_settings['corr_across']\n",
    "    # 같은 Factor 클러스터와 상관관계는 corr_within 대체함\n",
    "    corr_mat[corr_mat == 1] = sim_settings['corr_within']\n",
    "    np.fill_diagonal(corr_mat, 1)\n",
    "        \n",
    "    # Sigma matrix\n",
    "    sigma = (sim_settings['se'] ** 2) * corr_mat\n",
    "    \n",
    "    # Predefine variables\n",
    "    # 요인 개수만큼 생성 \n",
    "    alpha_0_vec = np.full(sim_settings['n'], sim_settings['alpha_0'])\n",
    "\n",
    "    i_n = np.eye(sim_settings['n'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "        \n",
    "    # Preallocate alpha noise\n",
    "    alpha_noise = np.random.multivariate_normal(mean=np.zeros(sim_settings['n']), cov=sigma, size=sim_settings['n_sims'])\n",
    "    \n",
    "    sim_results = []\n",
    "    \n",
    "    for s in range(sim_settings['n_sims']):\n",
    "        # Omega matrix\n",
    "        omega = np.dot(m, m.T) * (tau_c ** 2) + i_n * (tau_w ** 2)\n",
    "        \n",
    "        # True alphas\n",
    "        alpha_c = np.random.normal(0, tau_c, sim_settings['clusters'])\n",
    "        alpha_w = np.random.normal(0, tau_w, sim_settings['n'])\n",
    "        alpha_true = alpha_0_vec + np.dot(m, alpha_c) + alpha_w\n",
    "        alpha_hat = alpha_true + alpha_noise[s]\n",
    "        \n",
    "        # Posterior variance and alpha\n",
    "        post_var = inv(inv(omega) + inv(sigma))\n",
    "        post_alpha = np.dot(post_var, np.dot(inv(omega), alpha_0_vec) + np.dot(inv(sigma), alpha_hat))\n",
    "        \n",
    "        # Empirical Bayes results\n",
    "        post_alpha_z = post_alpha / np.sqrt(np.diag(post_var))\n",
    "        post_alpha_p = 2 * norm.sf(np.abs(post_alpha_z))\n",
    "        eb = pd.DataFrame({\n",
    "            'type': 'eb',\n",
    "            'true_alpha': alpha_true,\n",
    "            'z': post_alpha_z,\n",
    "            'p': post_alpha_p\n",
    "        })\n",
    "        \n",
    "        # OLS results\n",
    "        ols_z = alpha_hat / np.sqrt(np.diag(sigma))\n",
    "        ols_p = 2 * norm.sf(np.abs(ols_z))\n",
    "        ols = pd.DataFrame({\n",
    "            'type': 'ols',\n",
    "            'true_alpha': alpha_true,\n",
    "            'z': ols_z,\n",
    "            'p': ols_p\n",
    "        })\n",
    "        \n",
    "        # Benjamini-Yekutieli correction\n",
    "        by = pd.DataFrame({\n",
    "            'type': 'by',\n",
    "            'true_alpha': alpha_true,\n",
    "            'z': ols_z,\n",
    "            'p': pd.Series(ols_p).transform(lambda x: pd.Series(x).rank(ascending=False) / len(x))  # BY p-value adjustment placeholder\n",
    "        })\n",
    "        \n",
    "        # Combine results\n",
    "        all_results = pd.concat([eb, ols, by], ignore_index=True)\n",
    "        all_results['sig'] = (all_results['z'] > 0) & (all_results['p'] < 0.025)\n",
    "        \n",
    "        summary = all_results.groupby('type').apply(lambda df: pd.Series({\n",
    "            'sim': s,\n",
    "            'n_disc': df['sig'].sum(),\n",
    "            'true_disc': (np.sign(df['true_alpha'][df['sig']]) == np.sign(df['z'][df['sig']])).sum(),\n",
    "            'false_disc': df['sig'].sum() - (np.sign(df['true_alpha'][df['sig']]) == np.sign(df['z'][df['sig']])).sum()\n",
    "        }), include_groups=False)\n",
    "        sim_results.append(summary)\n",
    "    \n",
    "    sim_results_df = pd.concat(sim_results)\n",
    "    summary_results = sim_results_df.groupby('type').apply(lambda df: pd.Series({\n",
    "        'fdr': np.mean(df['false_disc'] / df['n_disc']),\n",
    "        'n_disc': np.mean(df['n_disc']),\n",
    "        'false_disc': np.mean(df['false_disc']),\n",
    "        'true_disc': np.mean(df['true_disc']),\n",
    "        'tau_c': tau_c,\n",
    "        'tau_w': tau_w,\n",
    "        'n': len(df)\n",
    "    }))\n",
    "    summary_results['true_disc_rate'] = summary_results['true_disc'] / (sim_settings['n'] / 2)\n",
    "    results.append(summary_results)\n",
    "    \n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpus = int( multiprocessing.cpu_count() * 2 / 3)\n",
    "parallel = Parallel(n_jobs=n_cpus, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  22 out of  22 | elapsed:  2.6min finished\n"
     ]
    }
   ],
   "source": [
    "sim_result_list = parallel(delayed(sim_mt_control)(tau_c=i[0], tau_w=i[1], sim_settings=sim) for i in iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation = pd.concat(sim_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.to_csv(data_path / \"fdr_sim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
