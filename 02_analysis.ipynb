{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from pathlib import Path \n",
    "from settings import settings \n",
    "from scipy.linalg import eigh\n",
    "from scipy.stats import bootstrap\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = {\n",
    "    \"us\": [\"us\", \"hml\", 2],\n",
    "    \"developed\": [\"developed\", \"hml\", 2],\n",
    "    \"emerging\": [\"emerging\", \"hml\", 2],\n",
    "    \"all\": [[\"us\", \"developed\", \"emerging\"], \"hml\", 3],\n",
    "    \"world\": [\"world\", \"hml\", 2],\n",
    "    \"world_ex_us\": [\"world_ex_us\", \"hml\", 2],\n",
    "    \"us_mega\": [\"us\", \"cmp\", 2, \"mega\"],\n",
    "    \"us_large\": [\"us\", \"cmp\", 2, \"large\"],\n",
    "    \"us_small\": [\"us\", \"cmp\", 2, \"small\"],\n",
    "    \"us_micro\": [\"us\", \"cmp\", 2, \"micro\"],\n",
    "    \"us_nano\": [\"us\", \"cmp\", 2, \"nano\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_pfs_cmp = pd.read_parquet(data_path / \"regional_pfs_cmp.parquet\") \n",
    "regional_pfs = pd.read_parquet(data_path / \"regional_pfs.parquet\")\n",
    "cluster_labels = pd.read_parquet(data_path / \"cluster_labels.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the empirical Bayes estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eb_prepare(data, scale_alphas, overlapping):\n",
    "    if overlapping:\n",
    "        data['obs'] = data.groupby(['region', 'characteristic'])['region'].transform('size')\n",
    "        data = data.loc[data.groupby(['region', 'characteristic'])['obs'].idxmax()]\n",
    "        data = data.drop(columns=['obs'])\n",
    "\n",
    "    data = data.copy()\n",
    "    data.loc[:, \"ret_neu\"] = data.groupby(['region', 'characteristic'], group_keys=False)[[\"ret\", \"mkt_vw_exc\"]].apply(lambda x: x[\"ret\"] - x['ret'].cov(x[\"mkt_vw_exc\"]) / x[\"mkt_vw_exc\"].var() * x[\"mkt_vw_exc\"])\n",
    "    data.loc[:, \"ret_neu\"] *= 100\n",
    "\n",
    "    scaling_fct = np.sqrt(10**2 / 12) / data.groupby(['region', 'characteristic'])['ret_neu'].transform('std')\n",
    "    data.loc[:, 'ret_neu_scaled'] = data['ret_neu'] * scaling_fct\n",
    "    data['name_wide'] = data['characteristic'] + '__' + data['region']\n",
    "\n",
    "    if scale_alphas:\n",
    "        data_wide = data.pivot(index='eom', columns='name_wide', values='ret_neu_scaled')\n",
    "    else:\n",
    "        data_wide = data.pivot(index='eom', columns='name_wide', values='ret_neu')\n",
    "    return {\n",
    "        \"long\": data, \n",
    "        \"wide\": data_wide\n",
    "    }\n",
    "\n",
    "\n",
    "def block_cluster_func(cor_mat: pd.DataFrame, cl_labels: pd.DataFrame):\n",
    "    cor_mat = cor_mat.copy()\n",
    "    cor_mat.index.name = \"index\"\n",
    "    cl_labels = cl_labels.copy()\n",
    "\n",
    "    __cor_long = cor_mat.reset_index().melt(id_vars='index', var_name='char2', value_name='cor') \n",
    "    # char: 요인이름과 region을 분리\n",
    "    __cor_long[['char2', 'region2']] = __cor_long['char2'].str.split('__', expand=True)\n",
    "    __cor_long[['char1', 'region1']] = __cor_long['index'].str.split('__', expand=True)\n",
    "\n",
    "    # 요인별 cluster 이름을 추가\n",
    "    __cor_long = __cor_long.merge(cl_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl1'}), left_on='char1', right_on='characteristic', how='left')\n",
    "    __cor_long = __cor_long.merge(cl_labels[['characteristic', 'hcl_label']].rename(columns={'hcl_label': 'hcl2'}), left_on='char2', right_on='characteristic', how='left')\n",
    "\n",
    "    # 개별 요인이 포함돼 있는 클러스터와 region을 합침\n",
    "    __cor_long['hclreg1'] = __cor_long['hcl1'] + '__' + __cor_long['region1']\n",
    "    __cor_long['hclreg2'] = __cor_long['hcl2'] + '__' + __cor_long['region2']\n",
    "\n",
    "    # Create hcl_pair column\n",
    "\n",
    "    __cor_long['hcl_pair'] = __cor_long.apply(lambda row: '_x_'.join(sorted([row['hclreg1'], row['hclreg2']])), axis=1)    \n",
    "    __cor_long['name1'] = __cor_long['char1'] + '__' + __cor_long['region1']\n",
    "    __cor_long['name2'] = __cor_long['char2'] + '__' + __cor_long['region2']\n",
    "\n",
    "    # 같은 thema안에서 correlation의 평균\n",
    "    __cluster_wise_cor = __cor_long[__cor_long['name1'] != __cor_long['name2']].groupby('hcl_pair')['cor'].mean().reset_index(name='cor_avg')    \n",
    "    __cor_long = __cor_long.merge(__cluster_wise_cor, on='hcl_pair', how='left') \n",
    "    __cor_long['cor_avg'] = np.where(__cor_long['name1'] == __cor_long['name2'], 1, __cor_long['cor_avg']) \n",
    "    __cluster_block_cor_matrix = __cor_long.pivot(index='name1', columns='name2', values='cor_avg') \n",
    "    return __cluster_block_cor_matrix     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_bayes(\n",
    "        data: pd.DataFrame,\n",
    "        cluster_labels: pd.DataFrame, \n",
    "        min_obs=5*12, \n",
    "        fix_alpha=False,\n",
    "        bs_cov=False,\n",
    "        cor_type=\"sample\",\n",
    "        shrinkage=0,\n",
    "        layers=3,\n",
    "        bs_samples=10000,\n",
    "        seed=None,\n",
    "        priors=None,\n",
    "        sigma=None,\n",
    "        plot=True\n",
    "    ):\n",
    "    __data = data.copy()\n",
    "    __cluster_labels = cluster_labels.copy()\n",
    "    __y_raw = __data['wide'].copy() \n",
    "    __obs = __y_raw.notna().sum() \n",
    "    __y = __y_raw.loc[:, __obs[__obs >= min_obs].index] \n",
    "\n",
    "    # Factor 개수\n",
    "    __n_fcts = __y.shape[1]                \n",
    "    # 모든 요인의 timeseries 수익률 평균\n",
    "    __y_mean = np.nanmean(__y, axis=0)  \n",
    "    if sigma is None:\n",
    "        if bs_cov:\n",
    "            __bs_full = []\n",
    "            for _ in range(bs_samples):\n",
    "                # 행을 랜덤하게 중복해서 sample함\n",
    "                __resampled_y = resample(__y, replace=True)\n",
    "                __mean_resampled = __resampled_y.mean() \n",
    "                __bs_full.append(__mean_resampled)\n",
    "            __bs_full = pd.DataFrame(__bs_full)\n",
    "            __alpha_sd = __bs_full.std()\n",
    "            __alpha_cor = __bs_full.corr()\n",
    "        else:\n",
    "            __y_sd = __y.std()\n",
    "            __alpha_sd = __y_sd / np.sqrt(__y.shape[0])\n",
    "            __alpha_cor = __y.corr().copy()\n",
    "        \n",
    "        # Apply shrinkage\n",
    "        __alpha_cor_shrunk = __alpha_cor * (1 - shrinkage) + np.eye(__n_fcts) * shrinkage\n",
    "\n",
    "        # Correlation Block Adjustment\n",
    "        if cor_type == \"sample\":\n",
    "            __alpha_cor_adj = __alpha_cor_shrunk.copy()\n",
    "        elif cor_type == \"block_clusters\":\n",
    "            __alpha_cor_adj = block_cluster_func(__alpha_cor_shrunk, __cluster_labels)\n",
    "        sigma = np.diag(__alpha_sd) @ __alpha_cor_adj.values @ np.diag(__alpha_sd)\n",
    "        sigma = pd.DataFrame(sigma, index=__alpha_cor_adj.columns, columns=__alpha_cor_adj.columns)                      \n",
    "    else:\n",
    "        __alpha_sd = np.sqrt(np.diag(sigma))\n",
    "\n",
    "    # value는 요인별 수익률의 평균 \n",
    "    __cm = pd.DataFrame({'char_reg': __data['wide'].columns, 'value': __y_mean})  # Assuming the first column is 'eom'\n",
    "    __cm['characteristic'] = __cm['char_reg'].str.split('__').str[0]\n",
    "    __cm = __cm.merge(__cluster_labels, on=\"characteristic\", how=\"left\")    \n",
    "\n",
    "    __m = pd.get_dummies(__cm[\"hcl_label\"]).astype(int).values    \n",
    "    # Cluster에 얼마나 속하는지 알 수 있다. \n",
    "    __mm = __m @ __m.T\n",
    "\n",
    "    __z = pd.get_dummies(__cm[\"characteristic\"]).astype(int).values \n",
    "    __zz = __z @ __z.T    \n",
    "\n",
    "    # Thmea 개수\n",
    "    __n_cl = __m.shape[1]\n",
    "    # Factor 개수 \n",
    "    __n_s = __z.shape[1]\n",
    "\n",
    "    __starting_values = __cm.groupby('hcl_label').agg(\n",
    "            # 요인의 개수 \n",
    "            n_s=('value', 'size'),\n",
    "            # 테마별 요인 수익률의 평균\n",
    "            signal_mean=('value', 'mean'),\n",
    "            # 테마별 요인 수익률의 표준편차 \n",
    "            signal_sd=('value', 'std')\n",
    "        ).groupby('hcl_label').agg(\n",
    "            n_c=('n_s', 'sum'),\n",
    "            cl_mean=('signal_mean', 'mean'),\n",
    "            cl_sd=('signal_mean', 'std'),\n",
    "            # Thema에 속하는 요인들의 표준편차의 평균\n",
    "            cl_signal_within=('signal_sd', 'mean')\n",
    "        ).reset_index()\n",
    "    \n",
    "    \n",
    "    if fix_alpha:\n",
    "        __sd_all = np.sqrt(np.sum(__y_mean ** 2) / (len(__y_mean) - 1))\n",
    "    else:\n",
    "        __sd_all = np.std(__y_mean)   \n",
    "\n",
    "    return {\n",
    "        \"sigma\": sigma, \n",
    "        \"alpha_sd\": __alpha_sd, \n",
    "        'alpha_cor_shrunk': __alpha_cor_shrunk,\n",
    "        'alpha_cor_adj': __alpha_cor_adj,\n",
    "        'starting_values': __starting_values,\n",
    "        'sd_all': __sd_all, \n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'us': ['us', 'hml', 2],\n",
       " 'developed': ['developed', 'hml', 2],\n",
       " 'emerging': ['emerging', 'hml', 2],\n",
       " 'all': [['us', 'developed', 'emerging'], 'hml', 3],\n",
       " 'world': ['world', 'hml', 2],\n",
       " 'world_ex_us': ['world_ex_us', 'hml', 2],\n",
       " 'us_mega': ['us', 'cmp', 2, 'mega'],\n",
       " 'us_large': ['us', 'cmp', 2, 'large'],\n",
       " 'us_small': ['us', 'cmp', 2, 'small'],\n",
       " 'us_micro': ['us', 'cmp', 2, 'micro'],\n",
       " 'us_nano': ['us', 'cmp', 2, 'nano']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us\n"
     ]
    }
   ],
   "source": [
    "eb_est = {}\n",
    "for key, x in search_list.items():\n",
    "    print(f\"Region: {x[0]}\")\n",
    "    regions = x[0]\n",
    "    \n",
    "    # Select the appropriate data\n",
    "    if x[1] == \"cmp\":\n",
    "        base_data = regional_pfs_cmp[regional_pfs_cmp['size_grp'] == x[3]].copy()\n",
    "    elif x[1] == \"hml\":\n",
    "        base_data = regional_pfs.copy()\n",
    "\n",
    "    if isinstance(regions, str):\n",
    "        regions = [regions]\n",
    "\n",
    "    data = base_data[(base_data['eom'] >= settings['start_date']) & (base_data['eom'] <= settings['end_date']) & (base_data['region'].isin(regions))]\n",
    "    data = eb_prepare(data, scale_alphas=settings['eb']['scale_alpha'], overlapping=settings['eb']['overlapping'])\n",
    "    op = emp_bayes(\n",
    "        data=data, \n",
    "        cluster_labels=cluster_labels, \n",
    "        min_obs=settings['eb']['min_obs'], \n",
    "        fix_alpha=settings['eb']['fix_alpha'], \n",
    "        bs_cov=settings['eb']['bs_cov'],\n",
    "        layers=x[2], \n",
    "        shrinkage=settings['eb']['shrinkage'], \n",
    "        cor_type=settings['eb']['cor_type'], \n",
    "        bs_samples=settings['eb']['bs_samples'], \n",
    "        seed=settings['seed'], \n",
    "        sigma = None,\n",
    "    )\n",
    "    eb_est[key] = op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
